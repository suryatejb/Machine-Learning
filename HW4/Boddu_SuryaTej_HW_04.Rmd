---
title: "INFSCI 2595 Fall 2025 Homework: 04"
subtitle: "Assigned September 18, 2025; Due: September 24, 2025"
author: "Surya Tej Boddu"
date: "Submission time: September 24, 2025 at 11:59PM EST"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Collaborators

Include the names of your collaborators here.  

## Overview

This assignment is focused on the mathematics of likelihoods, priors, and posteriors. You will work with binomially distributed data in this assignment. You must perform calculations in R using for-loops, functions, and visualize your results using `ggplot2`. You must also perform derivations and type your expressions in LaTeX within equation blocks.  

### IMPORTANT!!!

Certain code chunks are created for you. Each code chunk has `eval=FALSE` set in the chunk options. You **MUST** change it to be `eval=TRUE` in order for the code chunks to be evaluated when rendering the document.  

You are free to add more code chunks if you would like.  

## Load packages

You will ONLY use functions from the `tidyverse` in this assignment.  

```{r, load_tidyverse}
library(tidyverse)
```

## Problem 01

Baseball has a rich history of quantitative analysis, even before the rise of advanced analytics techniques. Batting averages, slugging percentages, and other metrics have been used to evaluate player performance for over one hundred years. The batting average, BA, is calculated using the number of at bats, AB, and the successful number of hits, H. The batting average measures the proportion of at bats that a player successfully gets a hit. You can think of the number of hits as the number of **events** and the number of at bats as the number of **trials**.  

You will work with a sequence of at bats of a real Major League Baseball (MLB) player. This sequence is a small sample from the 2022 MLB season. You are not told who this player is so that way you cannot know for certain if the player is a "good" or "bad" hitter! All you are provided with is the small sample size provided below.  

The code chunk below populates the data using the encoding and format discussed in lecture. Each observation (element) of the vector `x` corresponds to an individual at bat. The result, hit or out, is recorded as 1 for hit (**event**) and 0 for out (**non-event**).  

```{r, make_player_data}
x <- c(0, 0, 0, 0,
       0, 1, 1, 0,
       1, 0, 0, 0,
       0, 1, 1, 0, 1,
       1, 0, 0, 1,
       0, 0, 1, 0,
       1, 1, 1, 0, 0,
       0, 0, 1, 1)
```


### 1a)

**Calculate the average of the `x` vector.**  

**Display the result to the screen.**  

#### SOLUTION

```{r, solution_01a}
mean(x)
```

### 1b)

The player's batting results, hit or out, is a **binary outcome** which we will assume is a **Bernoulli** random variable. The likelihood function for each at bat (observation) is therefore the Bernoulli distribution. We will also assume the at bats are **independent**. The Bernoulli distribution consists of a single unknown parameter, $\mu$, the **event probability**. In the context of this application, the event probability represents the probability the player gets a hit.  

Having collected the player's data, you are tasked with estimating the player's hit probability. In lecture, we derived the Maximum Likelihood Estimate (MLE) for $\mu$.  

**Without going through any mathematical derivations, what is the MLE for this player's hit probability, based on the data provided?**  

#### SOLUTION

```{r, solution_01b}
mle <- sum(x) / length(x)
mle
```

### 1c)

**How does your result to 1a) compare to the result in 1b)?**  

#### SOLUTION

What do you think?  
The results are identical. This is because the sample average is the formula for the MLE of the event probability $\mu$ in a Bernoulli distribution. Both represent the proportion of successful hits in the provided sample.

### 1d)

Let's now dive into the Bernoulli distribution in greater detail.  

**Write the natural log of the Bernoulli distribution for a single at bat (observation) $x_n$ given the hit (event) probability, $\mu$.**  

**You MUST include at least several steps which simplify the expression using the properties of the natural log to receive full credit.**  

#### SOLUTION

I recommend using separate equation blocks for each line. You are not required to have all mathematical expressions in a single equation block.  
$$
P(x_n|\mu) = \mu^{x_n}(1-\mu)^{1-x_n}
$$
$$
\log\left(P(x_n|\mu)\right) = \log(\mu^{x_n}(1-\mu)^{1-x_n})
$$
$$
\log\left(P(x_n|\mu)\right) = \log(\mu^{x_n}) + \log((1-\mu)^{1-x_n})
$$
$$
\log\left(P(x_n|\mu)\right) = x_n\log\mu + (1-x_n)\log(1-\mu)
$$


### 1e)

The `log_bernoulli_pmf()` function is started for you in the code chunk below. It consists of two input arguments, `xobs`, and `prob`. The `xobs` argument is a numeric vector of observations of a binary variable encoded as 0 and 1. The `prob` argument is the event probability.  

**Complete the code chunk below by correctly calculating the log of the Bernoulli PMF. The function must return a numeric vector the same length as the `xobs` argument.**  

#### SOLUTION

```{r, define_logbernoulli, eval=TRUE}
log_bernoulli_pmf <- function(xobs, prob)
{
  xobs * log(prob) + (1 - xobs) * log(1 - prob)
}
```


### 1f)

Let's check the operation of your `log_bernoulli_pmf()` function.  

**Use separate function calls to the `log_bernoulli_pmf()` function to calculate the values associated with the 1st, 2nd, 3rd, 4th, and 5th observations of the `x` vector. Therefore, you must provide a single `x` observation to the function and do so 5 times.**  

**Use an event probability equal to 0.250 in each function call.**  

**Display the results to the screen.**  

#### SOLUTION

```{r, solution_01f}
for (i in 1:5) {
  cat("Observation", i, "->", log_bernoulli_pmf(x[i], 0.25), "\n")
}
```

### 1g)

The previous question focused on testing the function for a single observation. Let's now check the function works when multiple observations are provided.  

**Pass the first 5 elements of the `x` vector into the `log_bernoulli_pmf()` function. You must still use an event probability equal to 0.250.**  

**Display the result to the screen.**  

**What is the length of the returned result? How do the values compare to the previous question where you called the function separately for each observation?**  

#### SOLUTION

Insert code chunks.  
```{r, solution_01g}
log_bernoulli_pmf(x[1:5], 0.25)
```
- Length: The returned result is a numeric vector of length 5.
- Comparison: The values in this vector are identical to the values printed in 1f. This confirms that the function (whether using the vectorized version or the for-loop version) can handle multiple observations at once.

## Problem 02

Now that you have practiced calculating the log Bernoulli PMF, let's consider the **joint distribution** of all observations. Remember that the joint distribution is the **likelihood function** for this application. It corresponds to the probability of the exact sequence of observed data given the assumptions. The likelihood of all $N$ observations is written in vector notation for you in the equation block below:  

$$ 
p \left(x_1, x_2, ... , x_n, , ..., x_{N-1}, x_N \mid \mu \right) = p \left( \mathbf{x} \mid \mu \right)
$$

### 2a)

**Write the expression for the "complete" log-likelihood assuming the observations are independent.**  

#### SOLUTION

Write your answer in equation blocks  
$$
\log\left(p(\mathbf{x}\mid\mu)\right) = \sum_{n=1}^{N}[x_n\log(\mu)+(1-x_n)\log(1-\mu)]
$$

### 2b)

**Calculate the "complete" log-likelihood for all observations in `x`. You must continue to use an event probability of 0.25.**  

**Display the result to the screen.**  

#### SOLUTION

```{r, solution_02b}
sum(log_bernoulli_pmf(x, 0.25))
```

### 2c)

You might be wondering, why are you using probabilities of 0.25 when you already calculated the MLE at the beginning of the assignment? We derived the MLE in lecture, but you will graphically find the MLE in this assignment. You must therefore calculate the log-likelihood for many candidate event probability values, graph the results, and visually identify the probability that maximizes the likelihood.  

You will do this to reinforce optimization concepts such as why the MLE corresponds to the first derivative equal to zero. This exercise will also introduce visualizing curvature, an important concept we will discuss in greater depth later. Thus, the next few questions are laying the foundation for more complicated optimization problems such as training neural networks.  

You must call the `log_bernoulli_pmf()` function for many potential probability **candidate values**. You will use for-loops to accomplish the iteration procedure. A for-loop is not the most efficient approach to accomplishing this task. We will see more efficient methods soon. For now, the basic for-loop will demonstrate the key concepts.  

However, we need to setup the book keeping before we can iterate. We need the candidate probability values defined before anything else can happen.  

**Define a candidate grid of event probability values as a numeric vector using the `seq()` function. Specify the `from` argument to be 0.025 and the `to` argument to be `0.975`. Create the vector such that 251 evenly spaced values are between the bounds.**  

**Assign the vector the `mu_grid` object.**  

#### SOLUTION

```{r, solution_2c, eval=TRUE}
mu_grid <- seq(from = 0.025, to = 0.975, length.out = 251)
mu_grid
```


### 2d)

The basic structure of a for-loop in `R` is shown in the code chunk below. This simple for-loop simply prints the value of the *iterating variable* `n` to the screen. The `for` keyword is used to begin the for-loop. We must specify the iterating variable and the **sequence** the iterating variable is **in** within parentheses, `()`. The sequence in the example below is a vector starting at 1 and ending at 4.  

```{r, example_for_loop}
for( n in 1:4 ){
  print( n )
}
```

When we wish to calculate values and store them as elements within a larger object within a for-loop, it is best to first *initialize* the object with the appropriate size. This variable `example_vector` is initialized with `NA` (missing values) using the `rep()` function 10 times. Notice that the data type conversion function `as.numeric()` is used to ensure the initialized object is numeric.  

```{r, initialize_example_vector}
example_vector <- rep( as.numeric(NA), 10 )

example_vector %>% class()
```

To confirm the `example_vector` object contains only missing values.  

```{r, show_example_vector_elements}
example_vector
```

We can create a sequence of integers from 1 to the length of `example_vector` using the `seq_along()` function, as shown below. The `seq_along()` function is a useful programmatic approach to creating a vector of integers useful for iteration.  

```{r, show_seq_along_example}
seq_along(example_vector)
```

We can now iterate the elements of `example_vector` and populate the elements as desired. The simple example shown below simply sets each element of `example_vector` equal to the square of the element index.  

```{r, populate_example_vector_with_for_loop}
for( n in seq_along(example_vector) ){
  example_vector[n] <- n ^ 2
}
```


The `example_vector` object is displayed below to show it no longer contains missings.  

```{r, show_completed_example_vector}
example_vector
```

Obviously this simple example does not require a for-loop. We could have reached the same result by doing the following:  

```{r, check_result_for_example}
(1:10)^2
```

However, the point was to demonstrate the key ingredients of populating elements of an object within a for-loop. We must:  

* initialize the object to the appropriate size  
* iterate over the sequence of elements in the object  
* perform the necessary calculation and assign result to the object's element  

**You will follow the above steps in order to calculate the log-likelihood associated with the `x` vector for all candidate event probability values contained in the `mu_grid` vector. You must assign the result to the `log_lik_xa` object and that object must have the same length as `mu_grid`.**  

**You will still assume that all observations are independent.**  

*NOTE*: You must use a for-loop for this problem. We will make use of **functional programming** techniques to streamline this calculation later in the semester.  

#### SOLUTION

Add as many code chunks as you feel are necessary.  
```{r, solution_02d}
log_lik_xa = rep(as.numeric(NA), length(mu_grid))

for(i in seq_along(mu_grid)){
  log_lik_xa[i] = sum(log_bernoulli_pmf(x, mu_grid[i]))
}

length(log_lik_xa)
head(log_lik_xa)
```

### 2e)

The code chunk below is completed for you. It assigns the `mu_grid` and `log_lik_xa` vectors as data variables (columns) within a tibble, `xa_results`. The code chunk below is not evaluated by default.  

```{r, make_results_tibble_xa, eval=TRUE}
xa_results <- tibble::tibble(
  mu = mu_grid,
  log_lik = log_lik_xa
)
```


**Plot the log-likelihood with respect to the event probability using a line plot with `ggplot2`. The line should be created with the `geom_line()` function.**  

#### SOLUTION

Insert code chunks here.  
```{r, solution_02e}
ggplot(xa_results, aes(x = mu, y = log_lik)) + geom_line()
```

### 2f)

**Create the same line plot as in the previous question, but add an additional layer with `geom_vline()` to show a vertical reference line. Set the `xintercept` argument within `geom_vline()` to the MLE you calculated in 1b).**  

#### SOLUTION

Insert code chunks here.  
```{r, solution_02f}
ggplot(xa_results, aes(x = mu, y = log_lik)) + 
  geom_line(size = 1.1) +
  geom_vline(xintercept = mean(x), color = "red", linetype = "dashed") +
  theme_bw() +
  labs(x = expression(mu), y = "Log-Likelihood")
```

### 2g)

**Describe the behavior of the log-likelihood with respect to the candidate event probability around the MLE in the plot shown in 2f). Does the curve look different near the MLE compared to other candidate values?**  

#### SOLUTION

What do you think?  
In the plot, the log-likelihood curve reaches its maximum at the MLE (the red dashed vertical line). Around this point, the curve is highest and relatively flat compared to farther away, which means that small changes in ðœ‡ near the MLE do not reduce the log-likelihood by much.
As we move away from the MLE in either direction, the curve drops off, so candidate values far from the MLE fit the observed data much worse. This creates a peak centered at the MLE, showing that the data strongly supports probabilities near that value.

## Problem 03

Let's now consider tackling the problem from the perspective of the more general Binomial distribution.  

### 3a)

The previous problems worked with the observations stored as 0s and 1s in the vector `x`. You must now summarize the observations. The Binomial distribution requires the number of events, or Hits in this case, and the number of trials, or At Bats in this case.  

**Calculate the number of hits and number of at bats for the sequence of observations stored in the vector `x`. Assign the results to the corresponding variables defined in the code chunk below.**  

#### SOLUTION

```{r, solution_03a, eval=TRUE}
player_hits <- sum(x)
player_atbats <- length(x)

player_hits
player_atbats
```

### 3b)

You examined the behavior of the log-likelihood when we formulated the problem as a sequence of independent Bernoulli trials. As discussed in lecture, the Binomial distribution assumes the observations are independent Bernoulli trials! Thus, it should not matter if we analyze the problem with the Bernoulli formulation or the Binomial formulation. Let's confirm that is indeed true!  

You will work with the log of the Binomial likelihood up to a normalizing constant. That means, you do not need to consider terms that do not directly involve the unknown event probability $\mu$. Dropping or ignoring the constant terms is also referred to as the **un-normalized** likelihood.  

**Write out the expression for the Binomial log-likelihood up to a normalizing constant for the number of hits $H$, given the number of at bats, $AB$, and the probability of a hit, $\mu$. The equation block is started for you, showing that the log-likelihood is just proportional to the expression on the right hand side.**  

#### SOLUTION

$$ 
\log \left( p \left( H \mid AB, \mu \right) \right) \propto H \log \mu + (AB-H) \log(1-\mu)
$$

### 3c)

Regardless of the formulation (Bernoulli vs Binomial), our goal is to **learn the event probability**. Thus, we still need to find the maximum likelihood estimate (MLE) for $\mu$. You graphically solved this for the Bernoulli formulation in Problem 02. Let's now graphically find the MLE with the Binomial formulation. However, you do not need to use for-loops to compile the data necessary to create the figure when using the Binomial formulation!  

**The code chunk below is started for you. A tibble (dataframe) is created with the data variable (column) `mu` assigned to the `mu_grid` object you created earlier. The tibble is passed to the `mutate()` function for you. You must create a new variable, `log_lik` within `mutate()` which is equal to the Binomial log-likelihood up to a normalizing constant. Thus, `log_lik` must equal the expression you wrote in 3a). Pipe the result into  `ggplot()` and map the `x` aesthetic `mu` and the `y` aesthetic to `log_lik`. Use a `geom_line()` to display those aesthetics with `size` assigned to 1.1. As a reference, include your calculated MLE on the probability with a `geom_vline()`. The `geom_vline()` displays a vertical line at a specified `xintercept` value. You do not need to place `xintercept` within the `aes()` function. Assign the `size`, `linetype`, and `color` arguments within `geom_vline()` to 1, 'dashed', and 'red', respectively.**  

#### SOLUTION

```{r, solution_03c, eval=TRUE}
tibble::tibble(
  mu = mu_grid
) %>% 
  mutate(
    log_lik = player_hits * log(mu) + (player_atbats - player_hits) * log(1 - mu)
  ) %>% 
  ggplot(aes(x = mu, y = log_lik)) +
  geom_line(size = 1.1) +
  geom_vline(xintercept = mean(x), size = 1, linetype = "dashed", color = "red")
```


### 3d)

**How does your figure in 3c) compare to your figure in 2f)?**  

#### SOLUTION

What do you think?  
The figures are identical in shape. This is because the Binomial log-likelihood (up to a normalizing constant) is mathematically equivalent to the Bernoulli "complete" log-likelihood. While the Bernoulli approach sums individual observations and the Binomial approach uses summary counts (H and AB), both result in the same functional form for the unknown parameter Î¼.

### 3e)

The un-normalized Binomial likelihood you wrote in 3b) and programmed in 3c) is missing the Binomial coefficient. The Binomial coefficient properly normalizes the values of the Binomial distribution. It is not critical for the **shape** of the log-likelihood but the normalizing constant is critical for calculating probabilities.  

Unless specified otherwise, you are allowed to existing functions for evaluating properly normalized densities or probability mass functions. For the Binomial distribution, the predefined function is `dbinom()`. It contains 4 input arguments: `x`, `size`, `prob`, and `log`. `x` is the number of observed events. `size` is the number of trials, so you can think of `size` as the Trial size. `prob` is the probability of observing the event. `log` is a Boolean, so it equals either `TRUE` or `FALSE`. It is a flag to specify whether to return the log of the probability, `log=TRUE`, or the probability `log=FALSE`. By default, if you do not specify `log` the `dbinom()` function assumes `log=FALSE`.  

You must use the `dbinom()` function to evaluate the log-Binomial likelihood for the player, similar to what you did in 3c). However, instead of manually typing the log-likelihood up to a normalizing constant, you may use the `dbinom()` function to properly evaluate the log-likelihood.  

**The code chunk below is started for you and is structured similar to that in 3c). A tibble (dataframe) is created with the data variable (column) `mu` assigned to the `mu_grid` object. The tibble is passed to the `mutate()` function for you. You must create a new variable, `log_lik` within `mutate()` which is equal to the Binomial log-likelihood. Use the `dbinom()` function to correctly calculate the Binomial log-likelihood. Pay close attention to the arguments of `dbinom()`. Pipe the result into  `ggplot()` and map the `x` aesthetic `mu` and the `y` aesthetic to `log_lik`. Use a `geom_line()` to display those aesthetics with `size` assigned to 1.1. As a reference, include your calculated MLE on the probability with a `geom_vline()`. The `geom_vline()` displays a vertical line at a specified `xintercept` value. You do not need to place `xintercept` within the `aes()` function. Assign the `size`, `linetype`, and `color` arguments within `geom_vline()` to 1, 'dashed', and 'red', respectively.**  

*HINT*: Do not forget to set the `log` flag appropriately!  

#### SOLUTION

```{r, solution_03e, eval=TRUE}
tibble::tibble(
  mu = mu_grid
) %>% 
  mutate(
    log_lik = dbinom(
      x = player_hits,
      size = player_atbats,
      prob = mu,
      log = TRUE
    )
  ) %>% 
  ggplot(aes(x = mu, y = log_lik)) +
  geom_line(size = 1.1) +
  geom_vline(xintercept = mean(x), size = 1, linetype = "dashed", color = "red")
```


### 3f)

**How does your figure in 3e) compare to the figures in 3c) and 2f)?**  

#### SOLUTION

What do you think?  
The figure in 3e is identical in shape and peak location (the MLE) to those in 3c and 2f. The only difference is the y-axis scale, which is shifted vertically. This shift is due to the inclusion of the Binomial coefficient in dbinom(), which acts as a constant that changes the absolute log-likelihood value but not the relative location of the maximum.

## Problem 04

You estimated the event probability (probability of a hit) by maximizing the likelihood. It's now time to use Bayesian methods to learn a **posterior** distribution for the unknown parameter. This distribution will fully represent everything we know about the parameter, based on data and our assumptions. You will summarize this distribution to describe the uncertainty in the parameter, and representative values such as the posterior mean and most probable value (the posterior mode).  

As discussed in lecture, Bayesian methods require **prior** distributions. These distributions represent what we believe about the unknowns. Priors enable combining expert opinion with the data in a controlled and consistent manner. The prior allows us to specify bounds or constraints on the parameter and thus prevents the learning process from being fooled by noise or small sample sizes.  

Your goal is to learn the unknown event (hit) probability. You must therefore specify a prior belief about the probability that a professional baseball player gets a hit. You will use a **Beta** distribution to encode the prior belief on the event probability. The Beta **shape** parameters control the location, width (uncertainty), and skew (asymmetry) of the Beta distribution. Encoding our prior belief therefore comes down to specifying the shape parameter values.  

Instead of focusing on how we should optimally decide those shape parameters, your task is to examine the influence of the prior belief on the posterior result. You will thus try out two different priors and compare the resulting posterior distributions. Determining the "most appropriate" prior is something we will discuss later in the semester.  

The code chunk below defines two sets of shape parameters. The uniform set with both shape parameters equal to 1, and the "informative" set which to different values. Both sets refer to the first shape parameter as $a$ and the second shape parameter as $b$. The R function `dbeta()` refers to $a$ as the `shape1` argument and $b$ as the `shape2` argument.  

```{r, define_prior_shapes}
a_uniform <- 1
b_uniform <- 1

a_inform <- 10
b_inform <- 30
```


### 4a)

**What is the prior number of trials associated with the two sets of shape parameters?**  

#### SOLUTION

Insert code chunks to answer the question. 
```{r, solution_04a}
prior_trials_uniform <- a_uniform + b_uniform
prior_trials_inform  <- a_inform + b_inform

prior_trials_uniform
prior_trials_inform
```

### 4b)

**What is the prior expected value (mean) for the event (hit) probability associated with the two sets of shape parameters?**  

#### SOLUTION

Insert code chunks to answer the question.  
```{r, solution_04b}
prior_mean_uniform <- a_uniform / (a_uniform + b_uniform)
prior_mean_inform  <- a_inform  / (a_inform  + b_inform)

prior_mean_uniform
prior_mean_inform
```

### 4c)

You will visualize the prior distributions and are allowed to calculate the Beta density with the `dbeta()` function. The first argument to `dbeta()` is the probability parameter, `x`. The second argument to `dbeta()` is `shape1`, the third argument to `dbeta()` is `shape2`. You do not need to set any other argument to `dbeta()` for this question.  

**Plot the two types of prior distributions on the unknown event (hit) probability $\mu$. The Beta pdf can be evaluated with the `dbeta()` function. You must plot the prior with `ggplot2`. The code chunks below are started for you. The $\mu$ values are assigned to the `mu` column using the `mu_grid` object you defined earlier.**  

#### SOLUTION

Plot the uniform prior on $\mu$.  

```{r, solutioN_04c_a, eval=TRUE}
tibble::tibble(
  mu = mu_grid
) %>% 
  mutate(beta_pdf = dbeta(mu, shape1 = a_uniform, shape2 = b_uniform)) %>% 
  ggplot(aes(x = mu, y = beta_pdf)) +
  geom_line(size = 1.1) +
  theme_bw() +
  labs(x = expression(mu), y = "Beta PDF", title = "Uniform Prior")
```

Plot the informative prior on $\mu$.  

```{r, solution_04c_b, eval=TRUE}
tibble::tibble(
  mu = mu_grid
) %>% 
  mutate(beta_pdf = dbeta(mu, shape1 = a_inform, shape2 = b_inform)) %>% 
  ggplot(aes(x = mu, y = beta_pdf)) +
  geom_line(size = 1.1) +
  theme_bw() +
  labs(x = expression(mu), y = "Beta PDF", title = "Informative Prior")
```


### 4d)

**How do the two priors compare? Are there event probability values that are "ruled out" by either of the priors?**  

#### SOLUTION

What do you think?  

The two priors are very different:

- The uniform prior Beta(1,1) is flat, giving equal weight to all values of Î¼ between 0 and 1. It does not rule out any probability and represents complete lack of prior preference.

- The informative prior Beta(10,30) is concentrated around Î¼â‰ˆ0.25 with much smaller variance. It effectively downweights high values of Î¼ (e.g., above 0.5) and assigns most belief to probabilities near 0.2â€“0.3.

## Problem 05

Now that you have practiced working with the likelihood and the prior, it is time to study the posterior! However, before executing the analysis for the current baseball problem, you will manipulate the expressions to get a better understanding of the posterior distribution in this application.  

The previous problems in this assignment used notation consistent with the baseball example. However, you will use more generic nomenclature and syntax in this problem to be consistent with lecture. Thus, you will consider a Binomial likelihood with $m$ events out of $N$ trials. You are interested in learning the unknown event probability $\mu$ by combining the observations with the prior. You are using a Beta prior with prior shape parameters, $a$ and $b$.  

### 5a)

You previously wrote out the log-Binomial likelihood up to a normalizing constant in terms hits and at bats. You will rewrite that expression, but this time with the generic variables for the number of events $m$ out of a generic number of trials $N$.  

**Write out the un-normalized log-likelihood for the Binomial likelihood with $m$ events out of $N$ trials and unknown event probability $\mu$.**  

#### SOLUTION

The equation block is started for you below.  

$$ 
\log \left( p \left( m \mid N, \mu \right) \right) \propto m\log(\mu)+(N-m)\log(1-\mu)
$$

### 5b)

**Write the log-density of the Beta distribution up to a normalizing constant on the unknown event probability $\mu$ with shape parameters $a$ and $b$.**  

#### SOLUTION

The equation block is started for you below.  

$$ 
\log \left( p \left( \mu \mid a, b \right) \right) \propto (a-1)\log(\mu)+(b-1)\log(1-\mu)
$$

### 5c)

We already know that since the Beta is conjugate to the Binomial, the posterior distribution on the unknown event probability $\mu$ is also a Beta. You must practice working through the derivation of the updated shape parameters $a_{new}$ and $b_{new}$. The log-likelihood was written in Problem 5a) and the log-prior in Problem 5b). In this problem you must add the un-normalized log-likelihood to the un-normalized log-prior, then perform the required algebra to derive $a_{new}$ and $b_{new}$.  

**Derive the expressions for the updated or posterior Beta shape parameters. You must show all steps in the derivation. You are allowed to use multiple equation blocks if that's easier for you to type with.**  

#### SOLUTION

Write out your derivation below. An equation block is started for you, but you can add as many as you feel are necessary.  

The log-posterior is the sum of the un-normalized log-likelihood and the un-normalized log-prior:
$$ 
\log \left( p \left( \mu \mid m, N \right) \right) \propto \log \left( p \left( m \mid N, \mu \right) \right) + \log \left( p \left( \mu \mid a, b \right) \right)
$$
Substitute the expressions derived in 5a and 5b:
$$ 
\log \left( p \left( \mu \mid m, N \right) \right) \propto [m \log(\mu)+(N-m) \log(1-\mu)]+[(a-1) \log(\mu)+(b-1) \log(1-\mu)]
$$
$$
\log \left( p \left( \mu \mid m, N \right) \right) \propto (m+a-1) \log(\mu) + (N-m+b-1) \log(1-\mu)
$$
$$
\log (p(\mu \mid a_{new}, b_{new})) \propto (a_{new}-1)\log(\mu)+(b_{new}-1)\log(1-\mu)
$$
By matching the coefficients, we identify the updated shape parameters: a_new = a+m, b_new = b+N-m

### 5d)

Since the posterior distribution on $\mu$ is a Beta, a formula exists for the posterior mode (Max a-posterior estimate). However, you will practice deriving the posterior mode through differentiation of the un-normalized log-posterior. You can always double check your answer with the known formula for the mode of a Beta!  

**Derive the expression for the first derivative of the un-normalized log-posterior with respect to the unknown event probability $\mu$. Write out the derivative in terms of the updated shape parameters $a_{new}$ and $b_{new}$.**  

#### SOLUTION

You may add as many equation blocks as you feel are necessary. One is started for you below.  

$$ 
f(\mu) = (a_{new}-1)\log(\mu)+(b_{new}-1)\log(1-\mu)
$$

$$
\frac{\partial f(\mu)}{\partial \mu} = \frac{(a_{new}-1)}{\mu} - \frac{(b_{new}-1)}{1-\mu}
$$

### 5e)

**Set the derivative from your solution to Problem 5d) equal to zero and solve for the posterior mode of the unknown event probability. Denote the posterior mode as $\mu_{MAP}$.**  

#### SOLUTION

To find the mode, we set our derivative result to zero:
$$ 
\frac{(a_{new}-1)(1-\mu_{MAP})+(1-b_{new})\mu_{MAP}}{\mu_{MAP}(1-\mu_{MAP})} = 0
$$
$$
(2-a_{new}-b_{new})\mu_{MAP}-1+a_{new}=0
$$
The final expression for the posterior mode (MAP) is:
$$
\mu_{MAP} = \frac{a_{new}-1}{b_{new}+a_{new}-2} = \frac{m+a-1}{N+a+b-2}
$$

## Problem 06

Now that you've worked with the posterior Beta in greater detail, it is time to execute the Bayesian analysis for the baseball problem.  

As a reminder, there are two sets of prior shape parameters. The uniform prior is defined by `a_uniform` and `b_uniform`. The informative prior is defined by `a_inform` and `b_inform`. The observations are stored in the variables `player_hits` and `player_atbats`.  

### 6a)

**Calculate the updated or posterior shape parameters, $a_{new}$ and $b_{new}$, using the observations and the two sets of prior shape parameters.**  

**This problem is open ended, you are free to make the calculations anyway you like. However, you must display the updated shape parameters to the screen. Your results should be clearly marked as to which prior the posterior shape parameters are associated with.**  

#### SOLUTION

```{r, solution_06a}
player_hits <- sum(x)
player_atbats <- length(x)

a_new_uniform <- a_uniform + player_hits
b_new_uniform <- b_uniform + (player_atbats - player_hits)

a_new_inform <- a_inform + player_hits
b_new_inform <- b_inform + (player_atbats - player_hits)

cat("Uniform prior Beta(1,1) posterior:\n")
cat("a_new_uniform =", a_new_uniform, ", b_new_uniform =", b_new_uniform, "\n\n")

cat("Informative prior Beta(10,30) posterior:\n")
cat("a_new_inform =", a_new_inform, ", b_new_inform =", b_new_inform, "\n")
```

### 6b)

**Calculate the posterior mean, mode, 5th percentile (0.05 quantile), and 95th percentile (0.95 quantile) for the posteriors associated with the two prior types. You should use your results from Problem 6a) for the updated or posterior beta shape parameters**  

**Display your results as a dataframe or tibble.**  

*NOTE*: The `qbeta()` function allows calculating the quantiles associated with a particular probability of interest.  

#### SOLUTION

```{r, solution_06b}
mean_uniform <- a_new_uniform / (a_new_uniform + b_new_uniform)
mode_uniform <- (a_new_uniform - 1) / (a_new_uniform + b_new_uniform - 2)
q05_uniform  <- qbeta(0.05, a_new_uniform, b_new_uniform)
q95_uniform  <- qbeta(0.95, a_new_uniform, b_new_uniform)

mean_inform <- a_new_inform / (a_new_inform + b_new_inform)
mode_inform <- (a_new_inform - 1) / (a_new_inform + b_new_inform - 2)
q05_inform  <- qbeta(0.05, a_new_inform, b_new_inform)
q95_inform  <- qbeta(0.95, a_new_inform, b_new_inform)

tibble::tibble(
  Prior = c("Uniform", "Informative"),
  Mean = c(mean_uniform, mean_inform),
  Mode = c(mode_uniform, mode_inform),
  Q05  = c(q05_uniform, q05_inform),
  Q95  = c(q95_uniform, q95_inform)
)
```

### 6c)

Problem 6b) required that you summarize the posterior Beta distribution. This problem requires that you visualize the posterior Beta density associated with the two prior types. You will create the visualizations similiar to what you did in 4c), but you must use the updated or posterior shape parameters, instead of the prior shape parameters.  

**Plot the two posterior distributions on the unknown event (hit) probability $\mu$. The Beta pdf can be evaluated with the `dbeta()` function. You must plot the posterior with `ggplot2`. The code chunks below are started for you. The $\mu$ values are assigned to the `mu` column using the `mu_grid` object you defined earlier. Use the posterior shape parameters you calculated in 6a) in the appropriate code chunk below.**  

#### SOLUTION

Plot the posterior Beta associated with the uniform prior.  

```{r, solutioN_06c_a, eval=TRUE}
tibble::tibble(
  mu = mu_grid
) %>% 
  mutate(beta_pdf = dbeta(mu, shape1 = a_new_uniform, shape2 = b_new_uniform)) %>% 
  ggplot(aes(x = mu, y = beta_pdf)) +
  geom_line(linewidth = 1.1)
```

Plot the posterior Beta associated with the informative prior.  

```{r, solution_06c_b, eval=TRUE}
tibble::tibble(
  mu = mu_grid
) %>% 
  mutate(beta_pdf = dbeta(mu, shape1 = a_new_inform, shape2 = b_new_inform)) %>% 
  ggplot(aes(x = mu, y = beta_pdf)) +
  geom_line(linewidth = 1.1)
```


### 6d)

You have visualize the two posteriors and summarized them.  

**Based on your results, how would you describe the differences in the posterior belief based on the two sets of priors?**  

#### SOLUTION

What do you think?  

- Data Dominance in the Uniform Prior: Because the uniform prior (Beta(1,1)) represents only 2 prior observations, the posterior is driven by the observed data. It centers close to the sample hit rate (around 0.417) and remains relatively wide, which reflects higher uncertainty.

- Prior Influence and Shrinkage: The informative prior (Beta(10,30)) is much stronger, representing 40 prior observations. This pulls the posterior toward the prior mean of 0.25, resulting in lower posterior mean and mode values (around 0.32).

- Concentration: This informative prior is shrinking the estimate toward 0.25 and producing a tighter, more concentrated posterior. This shows how a strong prior can stabilize estimates when dealing with small sample sizes.

## Problem 07

The data provided to you at the beginning of the assignment is just a small sample of the at bats for this particular Major League Baseball player. The player has played in the MLB season June 2022 and has required many more at bats. You have evaluated the posterior based on the small sample size under two different prior assumptions. Let's now examine how the posterior behaves under larger sample sizes by using the data from the entire season.  

The code chunk below provides the season total hits (number of events) and at bats (number of trials) for this player (at least up to the creation of this assignment).  

```{r, give_season_data}
season_hits <- 62

season_atbats <- 284
```

You will use the same two sets of prior shape parameters as in the previous problem. However, you will use the larger sample size observations for this question.  

### 7a)

**Calculate the updated or posterior shape parameters, $a_{new}$ and $b_{new}$, using the season (larger sample size) observations and the two sets of prior shape parameters.**  

**This problem is open ended, you are free to make the calculations anyway you like. However, you must display the updated shape parameters to the screen. Your results should be clearly marked as to which prior the posterior shape parameters are associated with.**  

#### SOLUTION

```{r, solution_07a}
a_post_season_uniform <- a_uniform + season_hits
b_post_season_uniform <- b_uniform + (season_atbats - season_hits)

a_post_season_inform <- a_inform + season_hits
b_post_season_inform <- b_inform + (season_atbats - season_hits)

cat("Season Data: Uniform Prior Posterior\n")
cat("a_post_season_uniform:", a_post_season_uniform, "\n")
cat("b_post_season_uniform:", b_post_season_uniform, "\n\n")

cat("Season Data: Informative Prior Posterior\n")
cat("a_post_season_inform:", a_post_season_inform, "\n")
cat("b_post_season_inform:", b_post_season_inform, "\n")
```

### 7b)

**Calculate the posterior mean, mode, 5th percentile (0.05 quantile), and 95th percentile (0.95 quantile) for the posteriors associated with the two prior types. You should use your results from Problem 7a) for the updated or posterior beta shape parameters**  

**Display your results as a dataframe or tibble.**  

*NOTE*: The `qbeta()` function allows calculating the quantiles associated with a particular probability of interest.  

#### SOLUTION

```{r, solution_07b}
mean_season_uniform <- a_post_season_uniform / (a_post_season_uniform + b_post_season_uniform)
mode_season_uniform <- (a_post_season_uniform - 1) / (a_post_season_uniform + b_post_season_uniform - 2)
q05_season_uniform  <- qbeta(0.05, a_post_season_uniform, b_post_season_uniform)
q95_season_uniform  <- qbeta(0.95, a_post_season_uniform, b_post_season_uniform)

mean_season_inform <- a_post_season_inform / (a_post_season_inform + b_post_season_inform)
mode_season_inform <- (a_post_season_inform - 1) / (a_post_season_inform + b_post_season_inform - 2)
q05_season_inform  <- qbeta(0.05, a_post_season_inform, b_post_season_inform)
q95_season_inform  <- qbeta(0.95, a_post_season_inform, b_post_season_inform)

tibble::tibble(
  Prior = c("Uniform", "Informative"),
  Mean = c(mean_season_uniform, mean_season_inform),
  Mode = c(mode_season_uniform, mode_season_inform),
  Q05  = c(q05_season_uniform, q05_season_inform),
  Q95  = c(q95_season_uniform, q95_season_inform)
)
```

### 7c)

Problem 7b) required that you summarize the posterior Beta distribution. This problem requires that you visualize the posterior Beta density associated with the two prior types. You will create the visualizations similar to what you did in 4c), but you must use the updated or posterior shape parameters, instead of the prior shape parameters.  

**Plot the two posterior distributions on the unknown event (hit) probability $\mu$. The Beta pdf can be evaluated with the `dbeta()` function. You must plot the posterior with `ggplot2`. The code chunks below are started for you. The $\mu$ values are assigned to the `mu` column using the `mu_grid` object you defined earlier. Use the posterior shape parameters you calculated in 7a) in the appropriate code chunk below.**  

#### SOLUTION

Plot the posterior Beta associated with the uniform prior.  

```{r, solutioN_07c_a, eval=TRUE}
tibble::tibble(
  mu = mu_grid
) %>% 
  mutate(beta_pdf = dbeta(mu, shape1 = a_post_season_uniform, shape2 = b_post_season_uniform)) %>% 
  ggplot(aes(x = mu, y = beta_pdf)) +
  geom_line(linewidth = 1.1)
```

Plot the posterior Beta associated with the informative prior.  

```{r, solution_07c_b, eval=TRUE}
tibble::tibble(
  mu = mu_grid
) %>% 
  mutate(beta_pdf = dbeta(mu, shape1 = a_post_season_inform, shape2 = b_post_season_inform)) %>% 
  ggplot(aes(x = mu, y = beta_pdf)) +
  geom_line(linewidth = 1.1)
```


### 7d)

You examined the sensitivity of the posterior to two types of prior assumptions based on two sample sizes. One prior is uniform, while the other is "informative". One sample size was small, while the other was larger.  

**Describe the influence of the prior on the posterior when the sample size is small vs large.**  

#### SOLUTION

What do you think?  

- When the sample size is small, the prior has a strong influence on the posterior. Thatâ€™s why in Problem 06 the uniform prior produced a posterior centered closer to the sample hit rate, while the informative prior pulled the posterior toward its prior mean (around 0.25) and made it tighter.

- When the sample size is large (season data), the likelihood dominates and the posterior becomes much more concentrated, so both priors lead to very similar posterior means, modes, and credible intervals. So, with lots of data the prior matters much less, and the posterior is driven mostly by the observed hit rate.
